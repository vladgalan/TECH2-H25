{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Business cycle correlations\n",
    "\n",
    "For this exercise, you'll be using macroeconomic data from the folder `data/FRED`.\n",
    "\n",
    "1.  There are seven decade-specific files named `FRED_monthly_YYYY.csv` where `YYYY` identifies the decade by its first year (`YYYY` takes on the values 1950, 1960, ..., 2010). Write a loop that reads in all seven files as DataFrames and store them in a list.\n",
    "\n",
    "    *Hint:* Recall from the lecture that you should use `pd.read_csv(..., parse_dates=['DATE'])` to automatically parse strings stored in the `DATE` column as dates.\n",
    "2.  Use [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to concate these data sets into a single `DataFrame` and set the `DATE` column as the index.\n",
    "3.  You realize that your data does not include GDP since this variable is only reported at quarterly frequency.\n",
    "    Load the GDP data from the file `GDP.csv` and merge it with your monthly data using an _inner join_.\n",
    "4.  You want to compute how (percent) changes of the variables in your data correlate with percent changes in GDP.\n",
    "\n",
    "    1. Create a _new_ `DataFrame` which contains the percent changes in CPI and GDP (using \n",
    "    [`pct_change()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html),\n",
    "    and the absolute changes for the remaining variables (using \n",
    "    [`diff()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html)).\n",
    "    2.  Compute the correlation of the percent changes in GDP with the (percent) changes of all other variables using [`corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html). What does the sign and magnitude of the correlation coefficient tell you?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 2: Loading many data files\n",
    "\n",
    "In the previous exercise, you loaded the individual files by specifing an explicit list of file names. This can become tedious or infeasible if your data is spread across many files with varying file name patterns. Python offers the possibility to iterate over all files in a directory (for example, using [`os.listdir()`](https://docs.python.org/3/library/os.html#os.listdir)),\n",
    "or to iterate over files that match a pattern, for example using [`glob.glob()`](https://docs.python.org/3/library/glob.html).\n",
    "\n",
    "Repeat parts (1) and (2) from the previous exercise, but now iterate over the input files using \n",
    "[`glob.glob()`](https://docs.python.org/3/library/glob.html). You'll need to use a wildcard `*` and make sure to match only the relevant files in `data/FRED`, i.e., those that start with `FRED_monthly_1` or `FRED_monthly_2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 3: Weekly returns of the magnificent seven\n",
    "\n",
    "In this exercise, you are asked to analyze the weekly stockmarket returns\n",
    "of the so-called magnificent 7 which are some of the most successful tech companies \n",
    "of the last decades years:\n",
    "Apple (AAPL), Amazon (AMZN), Alphabet/Google (GOOGL), Meta (META), Microsoft (MSFT), Nvidia (NVDA), and Tesla (TSLA).\n",
    "\n",
    "The data for this exercise is located in the folder `data/stockmarket/`.\n",
    "\n",
    "1.  For each of the seven stocks listed above, there is a corresponding \n",
    "    CSV file in this directory (based on the ticker symbol).\n",
    "\n",
    "    1.  For each ticker symbol, load the corresponding CSV file and make sure \n",
    "        that the `Date` is set as the index.\n",
    "\n",
    "        The DataFrame has two columns, `Open` and `Close`, which contain the \n",
    "        opening and closing price for each trading day.\n",
    "\n",
    "    3.  Use [`resample()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)\n",
    "        to resample the daily data to a weekly frequency by specifying `resample('W')`,\n",
    "        and compute the weekly returns in percent:\n",
    "\n",
    "        $$\n",
    "        \\text{Weekly returns} = \\frac{\\text{Close price on last day} - \\text{Open price on first day}}{\\text{Open price on first day}} \\times 100\n",
    "        $$\n",
    "\n",
    "        *Hint:* You can obtain the first and last observation using the \n",
    "        [`first()`](https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.first.html) and \n",
    "        [`last()`](https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.last.html)\n",
    "        methods.\n",
    "\n",
    "    4.  Append these returns to a list so you can merge them into a single DataFrame later.\n",
    "\n",
    "2.  Merge the list of weekly returns you computed into a single DataFrame.\n",
    "    Keep only the intersection of dates available for all 7 stocks.\n",
    "\n",
    "    *Hint:* This can be achieved using either \n",
    "    [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html),\n",
    "    [`pd.merge()`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html), or \n",
    "    [`DataFrame.join()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.join.html).\n",
    "\n",
    "3.  Finally, you are interested in how the weekly returns are correlated across \n",
    "    the 7 stocks. \n",
    "\n",
    "    1.  Compute and report the pairwise correlations using \n",
    "        [DataFrame.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html).\n",
    "\n",
    "    2.  Create a figure with 7-by-7 subplots showing the pairwise scatter plots of weekly returns \n",
    "        for each combination of stocks.\n",
    "\n",
    "        You can do this either with the\n",
    "        [`scatter_matrix()`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function contained in `pandas.plotting`, \n",
    "        or manually build the figure using Matplotlib functions.\n",
    "\n",
    "    3.  **[Advanced]**\n",
    "        In each of the subplots, add a text that reports the pairwise correlation\n",
    "        for these stocks which you computed earlier.\n",
    "        (e.g., the correlation between returns on AAPL and AMZN is about 0.42,\n",
    "        so this text should be added to the subplot showing the \n",
    "        scatter plot of AAPL vs. AMZN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 4: Decade averages of macro time series\n",
    "\n",
    "\n",
    "For this exercise, you'll be using macroeconomic data from the folder `data/FRED`.\n",
    "\n",
    "1.  There are five files containing monthly observations on annual inflation (INFLATION), the Fed Funds rate (FEDFUNDS), the labor force participation rate (LFPART), the 1-year real interest rate (REALRATE) and the unemployment rate (UNRATE).\n",
    "\n",
    "    1.  Write a loop to import these files and store the individual DataFrames in a list.\n",
    "\n",
    "        *Hint:* Recall from the lecture that you should use \n",
    "        `pd.read_csv(..., parse_dates=['DATE'], index_col='DATE')` to automatically parse strings stored in the `DATE` column as dates and set the `DATE`\n",
    "        column as the index.\n",
    "\n",
    "    2.  Use \n",
    "        [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)\n",
    "        to concatenate this list of DataFrames along the column dimension\n",
    "        using an outer join (`join='outer'`) to obtain a merged data set.\n",
    "\n",
    "3.  You want to compute the average value of each variable by decade, but you want to include only decades without _any_ missing values for _all_ variables.\n",
    "\n",
    "    1.  Create a variable `Decade` which stores the decade (1940, 1950, ...) for each observation.\n",
    "\n",
    "        *Hint:* You should have set the `DATE` as the `DataFrame` index. Then you can access the calendar year using the attribute `df.index.year` which can be used to compute the decade.\n",
    "\n",
    "    2.  Create an indicator variable which takes on the value `True` \n",
    "        whenever all observations (all columns) for a given date are non-missing, and `False`\n",
    "        if at least one variable has a missing observation. \n",
    "\n",
    "    3.  Aggregate this indicator to decades using a\n",
    "    [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) so that the indicator takes on the value `True` whenever\n",
    "    _all_ variables in a given decade have no missing values, and `False`\n",
    "    otherwise.\n",
    "\n",
    "        *Hint:* You can use the \n",
    "        [`all()`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.all.html) aggregation for this.\n",
    "\n",
    "    4.  Merge this decade-level indicator data back into the original `DataFrame` (_many-to-one_ merge). \n",
    "4.  Using this indicator, drop all observations which are in a decade with missing values.\n",
    "5.  Compute the decade average for each variable.\n",
    "\n",
    "**Challenge**\n",
    "\n",
    "-   Your pandas guru friend claims that all the steps in 2.2 to 2.4 can be done with a single one-liner using [`transform()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transform.html). Can you come up with a solution?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Exercise 5: Merging additional Titanic data\n",
    "\n",
    "In this exercise, you'll be working with the the original Titanic data set in `titanic.csv` and additional (partly fictitious) information on passengers stored in `titanic-additional.csv`, both located in the `data/` folder.\n",
    "\n",
    "The goal of the exercise is to calculate the survival rates by country of residence (for this exercise we restrict ourselves to the UK, so these will be England, Scotland, etc.).\n",
    "\n",
    "1.  Load the `titanic.csv` and `titanic-additional.csv` into two DataFrames.\n",
    "\n",
    "    Inspect the columns contained in both data sets. As you can see, the original data contains the full name including the title\n",
    "    and potentially maiden name (for married women) in a single column.\n",
    "    The additional data contains this information in separate columns.\n",
    "    You want to merge these data sets, but you first need to create common keys in both DataFrames.\n",
    "\n",
    "2.  Since the only common information is the name, you'll need to extract the individual name components from the original DataFrame\n",
    "    and use these as merge keys.\n",
    "\n",
    "    Focusing only on men (who have names that are much easier to parse), split the `Name` column into the tokens \n",
    "    `Title`, `FirstName` and `LastName`, just like the columns in the second DataFrame.\n",
    "\n",
    "    *Hint:* This is the same task as in the last exercise in Workshop 2. You can just use your solution here.\n",
    "\n",
    "3.  Merge the two data sets based on the columns `Title`, `FirstName` and `LastName` you just created using a _left join_ (_one-to-one_ merge).\n",
    "    Tabulate the columns and the number of non-missing observations to make sure that merging worked. \n",
    "\n",
    "    *Note:* The additional data set contains address information only for passengers from the UK, so some of these fields will be missing.\n",
    "\n",
    "4.  You are now in a position to merge the country of residence (_many-to-one_ merge). Load the country data from `UK_post_codes.csv` which contains \n",
    "    the UK post code prefix (which you can ignore), the corresponding city, and the corresponding country.\n",
    "\n",
    "    Merge this data with your passenger data set using a _left join_ (what is the correct merge key?).\n",
    "\n",
    "5.  Tabulate the number of observations by `Country`, including the number of observations with missing `Country` (these are passengers residing outside the UK).\n",
    "\n",
    "    Finally, compute the mean survival rate by country."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECH2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
